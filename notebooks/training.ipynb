{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6087601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from time import time\n",
    "from os import path\n",
    "from torchvision import transforms\n",
    "import random\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195151e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DATABASE ###\n",
    "# Load the complete dataset\n",
    "OASIS_df = pd.read_csv(\n",
    "    '/Users/camille.brianceau/Downloads/OASIS-1_dataset/tsv_files/lab_1/OASIS_BIDS.tsv', sep='\\t',\n",
    "    usecols=['participant_id', 'session_id', 'alternative_id_1', 'sex',\n",
    "             'education_level', 'age_bl', 'diagnosis_bl', 'laterality', 'MMS',\n",
    "             'cdr_global', 'diagnosis']\n",
    ")\n",
    "# Show first items of the table\n",
    "print(OASIS_df.head())\n",
    "# First visual inspection\n",
    "_ = OASIS_df.hist(figsize=(16, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77403a03",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Study the characteristics of the AD & CN populations (age, sex, MMS, cdr_global)\n",
    "def characteristics_table(df, merged_df):\n",
    "    \"\"\"Creates a DataFrame that summarizes the characteristics of the DataFrame df\"\"\"\n",
    "    diagnoses = np.unique(df.diagnosis.values)\n",
    "    population_df = pd.DataFrame(index=diagnoses,\n",
    "                                columns=['N', 'age', '%sexF', 'education',\n",
    "                                         'MMS', 'CDR=0', 'CDR=0.5', 'CDR=1', 'CDR=2'])\n",
    "    merged_df = merged_df.set_index(['participant_id', 'session_id'], drop=True)\n",
    "    df = df.set_index(['participant_id', 'session_id'], drop=True)\n",
    "    sub_merged_df = merged_df.loc[df.index]\n",
    "\n",
    "    for diagnosis in population_df.index.values:\n",
    "        diagnosis_df = sub_merged_df[df.diagnosis == diagnosis]\n",
    "        population_df.loc[diagnosis, 'N'] = len(diagnosis_df)\n",
    "        # Age\n",
    "        mean_age = np.mean(diagnosis_df.age_bl)\n",
    "        std_age = np.std(diagnosis_df.age_bl)\n",
    "        population_df.loc[diagnosis, 'age'] = '%.1f ± %.1f' % (mean_age, std_age)\n",
    "        # Sex\n",
    "        population_df.loc[diagnosis, '%sexF'] = round((len(diagnosis_df[diagnosis_df.sex == 'F']) / len(diagnosis_df)) * 100, 1)\n",
    "        # Education level\n",
    "        mean_education_level = np.nanmean(diagnosis_df.education_level)\n",
    "        std_education_level = np.nanstd(diagnosis_df.education_level)\n",
    "        population_df.loc[diagnosis, 'education'] = '%.1f ± %.1f' % (mean_education_level, std_education_level)\n",
    "        # MMS\n",
    "        mean_MMS = np.mean(diagnosis_df.MMS)\n",
    "        std_MMS = np.std(diagnosis_df.MMS)\n",
    "        population_df.loc[diagnosis, 'MMS'] = '%.1f ± %.1f' % (mean_MMS, std_MMS)\n",
    "        # CDR\n",
    "        for value in ['0', '0.5', '1', '2']:\n",
    "          population_df.loc[diagnosis, 'CDR=%s' % value] = len(diagnosis_df[diagnosis_df.cdr_global == float(value)])\n",
    "\n",
    "    return population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931aaaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = characteristics_table(OASIS_df, OASIS_df)\n",
    "population_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9a511",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## PREPROCESSING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3c5e06",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38abe4b9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MRIDataset(Dataset):\n",
    "\n",
    "    def __init__(self, img_dir, data_df, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_dir (str): path to the CAPS directory containing preprocessed images\n",
    "            data_df (DataFrame): metadata of the population.\n",
    "                Columns include participant_id, session_id and diagnosis).\n",
    "            transform (callable): list of transforms applied on-the-fly, chained with torchvision.transforms.Compose.\n",
    "        \"\"\"\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.data_df = data_df\n",
    "        self.label_code = {\"AD\": 1, \"CN\": 0}\n",
    "\n",
    "        self.size = self[0]['image'].shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        diagnosis = self.data_df.loc[idx, 'diagnosis']\n",
    "        label = self.label_code[diagnosis]\n",
    "\n",
    "        participant_id = self.data_df.loc[idx, 'participant_id']\n",
    "        session_id = self.data_df.loc[idx, 'session_id']\n",
    "        filename = 'subjects/' + participant_id + '/' + session_id + '/' + \\\n",
    "          'deeplearning_prepare_data/image_based/custom/' + \\\n",
    "          participant_id + '_' + session_id + \\\n",
    "          '_T1w_segm-graymatter_space-Ixi549Space_modulated-off_probability.pt'\n",
    "\n",
    "        image = torch.load(path.join(self.img_dir, filename))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        sample = {'image': image, 'label': label,\n",
    "                  'participant_id': participant_id,\n",
    "                  'session_id': session_id}\n",
    "        return sample\n",
    "\n",
    "    def train(self):\n",
    "        self.transform.train()\n",
    "\n",
    "    def eval(self):\n",
    "        self.transform.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1854ec8",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CropLeftHC(object):\n",
    "    \"\"\"Crops the left hippocampus of a MRI non-linearly registered to MNI\"\"\"\n",
    "    def __init__(self, random_shift=0):\n",
    "        self.random_shift = random_shift\n",
    "        self.train_mode = True\n",
    "    def __call__(self, img):\n",
    "        if self.train_mode:\n",
    "            x = random.randint(-self.random_shift, self.random_shift)\n",
    "            y = random.randint(-self.random_shift, self.random_shift)\n",
    "            z = random.randint(-self.random_shift, self.random_shift)\n",
    "        else:\n",
    "            x, y, z = 0, 0, 0\n",
    "        return img[:, 25 + x:55 + x,\n",
    "                   50 + y:90 + y,\n",
    "                   27 + z:57 + z].clone()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee7ac1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class CropRightHC(object):\n",
    "    \"\"\"Crops the right hippocampus of a MRI non-linearly registered to MNI\"\"\"\n",
    "    def __init__(self, random_shift=0):\n",
    "        self.random_shift = random_shift\n",
    "        self.train_mode = True\n",
    "    def __call__(self, img):\n",
    "        if self.train_mode:\n",
    "            x = random.randint(-self.random_shift, self.random_shift)\n",
    "            y = random.randint(-self.random_shift, self.random_shift)\n",
    "            z = random.randint(-self.random_shift, self.random_shift)\n",
    "        else:\n",
    "            x, y, z = 0, 0, 0\n",
    "        return img[:, 65 + x:95 + x,\n",
    "                   50 + y:90 + y,\n",
    "                   27 + z:57 + z].clone()\n",
    "\n",
    "    def train(self):\n",
    "        self.train_mode = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.train_mode = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6a953c",
   "metadata": {},
   "source": [
    "## VISUALIZATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea46936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ddeeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject = 'sub-OASIS10003'\n",
    "preprocessed_pt = torch.load(f'/Users/camille.brianceau/Downloads/OASIS-1_dataset/CAPS/subjects/{subject}/ses-M00/' +\n",
    "                    f'deeplearning_prepare_data/image_based/custom/{subject}_ses-M00_' +\n",
    "                    'T1w_segm-graymatter_space-Ixi549Space_modulated-off_' +\n",
    "                    'probability.pt')\n",
    "raw_nii = nib.load(f'/Users/camille.brianceau/Downloads/OASIS-1_dataset/raw/{subject}_ses-M00_T1w.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233fab4d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "raw_np = raw_nii.get_fdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef5ea23",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def show_slices(slices):\n",
    "    \"\"\" Function to display a row of image slices \"\"\"\n",
    "    fig, axes = plt.subplots(1, len(slices))\n",
    "    for i, slice in enumerate(slices):\n",
    "        axes[i].imshow(slice.T, cmap=\"gray\", origin=\"lower\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5a2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = raw_np[:, :, 78]\n",
    "slice_1 = raw_np[122, :, :]\n",
    "slice_2 = raw_np[:, 173, :]\n",
    "show_slices([slice_0, rotate(slice_1, 90), rotate(slice_2, 90)])\n",
    "plt.suptitle(f'Slices of raw image of subject {subject}')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42798336",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = preprocessed_pt[0, 60, :, :]\n",
    "slice_1 = preprocessed_pt[0, :, 72, :]\n",
    "slice_2 = preprocessed_pt[0, :, :, 60]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(f'Center slices of preprocessed image of subject {subject}')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/4.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b6bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "leftHC_pt = CropLeftHC()(preprocessed_pt)\n",
    "slice_0 = leftHC_pt[0, 15, :, :]\n",
    "slice_1 = leftHC_pt[0, :, 20, :]\n",
    "slice_2 = leftHC_pt[0, :, :, 15]\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(f'Center slices of left HC of subject {subject}')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/3.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e4facd",
   "metadata": {},
   "source": [
    "## CROSS VALIDATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d02edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('/Users/camille.brianceau/Downloads/OASIS-1_dataset/tsv_files/lab_1/train.tsv', sep='\\t')\n",
    "valid_df = pd.read_csv('/Users/camille.brianceau/Downloads/OASIS-1_dataset/tsv_files/lab_1/validation.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef2d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_population_df = characteristics_table(train_df, OASIS_df)\n",
    "valid_population_df = characteristics_table(valid_df, OASIS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec7fbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train dataset:\\n {train_population_df}\\n\")\n",
    "print(f\"Validation dataset:\\n {valid_population_df}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cb6a0a",
   "metadata": {},
   "source": [
    "## MODEL ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a584c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = path.join('/Users/camille.brianceau/Downloads/OASIS-1_dataset', 'CAPS')\n",
    "batch_size=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5310f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dataset = MRIDataset(img_dir, OASIS_df, transform=CropLeftHC())\n",
    "example_dataloader = DataLoader(example_dataset, batch_size=batch_size, drop_last=True)\n",
    "for data in example_dataloader:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9568b432",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of Dataset output:\\n {example_dataset[0]['image'].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6ad9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of DataLoader output:\\n {data['image'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d555ad0",
   "metadata": {},
   "source": [
    "## CONVOLUTION ### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aa41fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa9de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv3d(8, 16, 3)\n",
    "print('Weights shape\\n', conv_layer.weight.shape)\n",
    "print()\n",
    "print('Bias shape\\n', conv_layer.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faae9f91",
   "metadata": {},
   "source": [
    "## BATCH NORMALIZATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c392449",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_layer = nn.BatchNorm3d(16)\n",
    "print('Gamma value\\n', batch_layer.state_dict()['weight'].shape)\n",
    "print()\n",
    "print('Beta value\\n', batch_layer.state_dict()['bias'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e826dc",
   "metadata": {},
   "source": [
    "## POOLING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9ad1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadMaxPool3d(nn.Module):\n",
    "    \"\"\"A MaxPooling module which deals with odd sizes with padding\"\"\"\n",
    "    def __init__(self, kernel_size, stride, return_indices=False, return_pad=False):\n",
    "        super(PadMaxPool3d, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.pool = nn.MaxPool3d(kernel_size, stride, return_indices=return_indices)\n",
    "        self.pad = nn.ConstantPad3d(padding=0, value=0)\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "\n",
    "    def set_new_return(self, return_indices=True, return_pad=True):\n",
    "        self.return_indices = return_indices\n",
    "        self.return_pad = return_pad\n",
    "        self.pool.return_indices = return_indices\n",
    "\n",
    "    def forward(self, f_maps):\n",
    "        coords = [self.stride - f_maps.size(i + 2) % self.stride for i in range(3)]\n",
    "        for i, coord in enumerate(coords):\n",
    "            if coord == self.stride:\n",
    "                coords[i] = 0\n",
    "\n",
    "        self.pad.padding = (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "\n",
    "        if self.return_indices:\n",
    "            output, indices = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, indices, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output, indices\n",
    "\n",
    "        else:\n",
    "            output = self.pool(self.pad(f_maps))\n",
    "\n",
    "            if self.return_pad:\n",
    "                return output, (coords[2], 0, coords[1], 0, coords[0], 0)\n",
    "            else:\n",
    "                return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a00d16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "33927146",
   "metadata": {},
   "source": [
    "### DROPOUT ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff19591",
   "metadata": {},
   "source": [
    "dropout = nn.Dropout(0.5)\n",
    "input_tensor = torch.rand(10)\n",
    "output_tensor = dropout(input_tensor)\n",
    "print(\"Input \\n\", input_tensor)\n",
    "print()\n",
    "print(\"Output \\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e156c71",
   "metadata": {},
   "source": [
    "### FULLY CONNECTED ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d4382c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "fc = nn.Linear(16, 2)\n",
    "print(\"Weights shape \\n\", fc.weight.shape)\n",
    "print()\n",
    "print(\"Bias shape \\n\", fc.bias.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf51d8f",
   "metadata": {},
   "source": [
    "## CUSTOM NETWORK ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11783c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To complete\n",
    "class CustomNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        self.convolutions = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, 3, padding=1),\n",
    "            # Size 8@30x40x30\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2),\n",
    "            # Size 8@15x20x15\n",
    "\n",
    "            nn.Conv3d(8, 16, 3, padding=1),\n",
    "            # Size 16@15x20x15\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2),\n",
    "            # Size 16@8x10x8)\n",
    "\n",
    "            nn.Conv3d(16, 32, 3, padding=1),\n",
    "            # Size 32@8x10x8\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2),\n",
    "            # Size 32@4x5x4\n",
    "\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(32 * 4 * 5 * 4, 2)\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convolutions(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97ef12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd6b3161",
   "metadata": {},
   "source": [
    "## TRAIN ET TEST ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb55fd7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    Method used to train a CNN\n",
    "\n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "        optimizer: (torch.optim) an optimization algorithm\n",
    "        n_epochs: (int) number of epochs performed during training\n",
    "\n",
    "    Returns:\n",
    "        best_model: (nn.Module) the trained neural network\n",
    "    \"\"\"\n",
    "    best_model = deepcopy(model)\n",
    "    train_best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loader.dataset.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # Retrieve mini-batch and put data on GPU with .cuda()\n",
    "            images, labels = data['image'], data['label']#.cuda(), data['label'].cuda()\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            # Loss computation\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Back-propagation (gradients computation)\n",
    "            loss.backward()\n",
    "            # Parameters update\n",
    "            optimizer.step()\n",
    "            # Erase previous gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        _, train_metrics = test(model, train_loader, criterion)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}: loss = {train_metrics['mean_loss']:.4f}, \"\n",
    "            f\"balanced accuracy = {train_metrics['balanced_accuracy']:.4f}\"\n",
    "            )\n",
    "\n",
    "        if train_metrics['mean_loss'] < train_best_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            train_best_loss = train_metrics['mean_loss']\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2678312e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, data_loader, criterion):\n",
    "    \"\"\"\n",
    "    Method used to test a CNN\n",
    "\n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "\n",
    "    Returns:\n",
    "        results_df: (DataFrame) the label predicted for every subject\n",
    "        results_metrics: (dict) a set of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader.dataset.eval()\n",
    "    columns = [\"participant_id\", \"proba0\", \"proba1\",\n",
    "               \"true_label\", \"predicted_label\"]\n",
    "    results_df = pd.DataFrame(columns=columns)\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            images, labels = data['image'], data['label'] #.cuda(), data['label'].cuda()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            probs = nn.Softmax(dim=1)(outputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            for idx, sub in enumerate(data['participant_id']):\n",
    "                row = [sub,\n",
    "                       probs[idx, 0].item(), probs[idx, 1].item(),\n",
    "                       labels[idx].item(), predicted[idx].item()]\n",
    "                row_df = pd.DataFrame([row], columns=columns)\n",
    "                results_df = pd.concat([results_df, row_df])\n",
    "\n",
    "    results_metrics = compute_metrics(results_df.true_label.values, results_df.predicted_label.values)\n",
    "    results_df.reset_index(inplace=True, drop=True)\n",
    "    results_metrics['mean_loss'] = total_loss / len(data_loader.dataset)\n",
    "\n",
    "    return results_df, results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7cacc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(ground_truth, prediction):\n",
    "    \"\"\"Computes the accuracy, sensitivity, specificity and balanced accuracy\"\"\"\n",
    "    tp = np.sum((prediction == 1) & (ground_truth == 1))\n",
    "    tn = np.sum((prediction == 0) & (ground_truth == 0))\n",
    "    fp = np.sum((prediction == 1) & (ground_truth == 0))\n",
    "    fn = np.sum((prediction == 0) & (ground_truth == 1))\n",
    "\n",
    "    metrics_dict = dict()\n",
    "    metrics_dict['accuracy'] = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "    # Sensitivity\n",
    "    if tp + fn != 0:\n",
    "        metrics_dict['sensitivity'] = tp / (tp + fn)\n",
    "    else:\n",
    "        metrics_dict['sensitivity'] = 0.0\n",
    "\n",
    "    # Specificity\n",
    "    if fp + tn != 0:\n",
    "        metrics_dict['specificity'] = tn / (fp + tn)\n",
    "    else:\n",
    "        metrics_dict['specificity'] = 0.0\n",
    "\n",
    "    metrics_dict['balanced_accuracy'] = (metrics_dict['sensitivity'] + metrics_dict['specificity']) / 2\n",
    "\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b2ec69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f44581b5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## TRAIN WITH LEFT HC ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e269eea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = path.join('/Users/camille.brianceau/Downloads/OASIS-1_dataset', 'CAPS')\n",
    "transform = CropLeftHC(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe793d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasetLeftHC = MRIDataset(img_dir, train_df, transform=transform)\n",
    "valid_datasetLeftHC = MRIDataset(img_dir, valid_df, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34af7469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try different learning rates\n",
    "learning_rate = 10**-4\n",
    "n_epochs = 30\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d7cbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the network on GPU\n",
    "modelLeftHC = CustomNetwork() #.cuda()\n",
    "train_loaderLeftHC = DataLoader(train_datasetLeftHC, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "# A high batch size improves test speed\n",
    "valid_loaderLeftHC = DataLoader(valid_datasetLeftHC, batch_size=32, shuffle=False, pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(modelLeftHC.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe9c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_modelLeftHC = train(modelLeftHC, train_loaderLeftHC, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3442e2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "valid_resultsLeftHC_df, valid_metricsLeftHC = test(best_modelLeftHC, valid_loaderLeftHC, criterion)\n",
    "train_resultsLeftHC_df, train_metricsLeftHC = test(best_modelLeftHC, train_loaderLeftHC, criterion)\n",
    "print(valid_metricsLeftHC)\n",
    "print(train_metricsLeftHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb0ccf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_resultsLeftHC_df = valid_resultsLeftHC_df.merge(OASIS_df, how='left', on='participant_id', sort=False)\n",
    "valid_resultsLeftHC_old_df = valid_resultsLeftHC_df[(valid_resultsLeftHC_df.age_bl >= 62)]\n",
    "#compute_metrics(valid_resultsLeftHC_old_df.true_label, valid_resultsLeftHC_old_df.predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea85796",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN WITH RIGHT HC ###\n",
    "img_dir = path.join('/Users/camille.brianceau/Downloads/OASIS-1_dataset', 'CAPS')\n",
    "transform = CropRightHC(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2d9ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasetRightHC = MRIDataset(img_dir, train_df, transform=transform)\n",
    "valid_datasetRightHC = MRIDataset(img_dir, valid_df, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8083600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10**-4\n",
    "n_epochs = 30\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b07942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the network on GPU\n",
    "modelRightHC = CustomNetwork() #.cuda()\n",
    "train_loaderRightHC = DataLoader(train_datasetRightHC, batch_size=batch_size, shuffle=True,  pin_memory=True)\n",
    "valid_loaderRightHC = DataLoader(valid_datasetRightHC, batch_size=32, shuffle=False,  pin_memory=True)\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(modelRightHC.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d9f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_modelRightHC = train(modelRightHC, train_loaderRightHC, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43167737",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_resultsRightHC_df, valid_metricsRightHC = test(best_modelRightHC, valid_loaderRightHC, criterion)\n",
    "train_resultsRightHC_df, train_metricsRightHC = test(best_modelRightHC, train_loaderRightHC, criterion)\n",
    "print(valid_metricsRightHC)\n",
    "print(train_metricsRightHC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0850a2bb",
   "metadata": {},
   "source": [
    "## SOFT VOTING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084320c4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def softvoting(leftHC_df, rightHC_df):\n",
    "    df1 = leftHC_df.set_index('participant_id', drop=True)\n",
    "    df2 = rightHC_df.set_index('participant_id', drop=True)\n",
    "    results_df = pd.DataFrame(index=df1.index.values,\n",
    "                              columns=['true_label', 'predicted_label',\n",
    "                                       'proba0', 'proba1'])\n",
    "    results_df.true_label = df1.true_label\n",
    "    # Compute predicted label and probabilities\n",
    "    results_df.proba1 = 0.5 * df1.proba1 + 0.5 * df2.proba1\n",
    "    results_df.proba0 = 0.5 * df1.proba0 + 0.5 * df2.proba0\n",
    "    results_df.predicted_label = (0.5 * df1.proba1 + 0.5 * df2.proba1 > 0.5).astype(int)\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53d2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_results = softvoting(valid_resultsLeftHC_df, valid_resultsRightHC_df)\n",
    "valid_metrics = compute_metrics(valid_results.true_label, valid_results.predicted_label)\n",
    "print(valid_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787916e5",
   "metadata": {},
   "source": [
    "## CLUSTERING ON AD & CN ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5643864",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CropMaxUnpool3d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super(CropMaxUnpool3d, self).__init__()\n",
    "        self.unpool = nn.MaxUnpool3d(kernel_size, stride)\n",
    "\n",
    "    def forward(self, f_maps, indices, padding=None):\n",
    "        output = self.unpool(f_maps, indices)\n",
    "        if padding is not None:\n",
    "            x1 = padding[4]\n",
    "            y1 = padding[2]\n",
    "            z1 = padding[0]\n",
    "            output = output[:, :, x1::, y1::, z1::]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8873fdfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "\n",
    "        # Initial size (30, 40, 30)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, 3, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (15, 20, 15)\n",
    "\n",
    "            nn.Conv3d(8, 16, 3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (8, 10, 8)\n",
    "\n",
    "            nn.Conv3d(16, 32, 3, padding=1),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.LeakyReLU(),\n",
    "            PadMaxPool3d(2, 2, return_indices=True, return_pad=True),\n",
    "            # Size (4, 5, 4)\n",
    "\n",
    "            nn.Conv3d(32, 1, 1),\n",
    "            # Size (4, 5, 4)\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose3d(1, 32, 1),\n",
    "            # Size (4, 5, 4)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(32, 16, 3, padding=1),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.LeakyReLU(),\n",
    "            # Size (8, 10, 8)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(16, 8, 3, padding=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.LeakyReLU(),\n",
    "            # Size (15, 20, 15)\n",
    "\n",
    "            CropMaxUnpool3d(2, 2),\n",
    "            nn.ConvTranspose3d(8, 1, 3, padding=1),\n",
    "            nn.BatchNorm3d(1),\n",
    "            nn.Sigmoid()\n",
    "            # Size (30, 40, 30)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        indices_list = []\n",
    "        pad_list = []\n",
    "        for layer in self.encoder:\n",
    "            if isinstance(layer, PadMaxPool3d):\n",
    "                x, indices, pad = layer(x)\n",
    "                indices_list.append(indices)\n",
    "                pad_list.append(pad)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        code = x.view(x.size(0), -1)\n",
    "        for layer in self.decoder:\n",
    "            if isinstance(layer, CropMaxUnpool3d):\n",
    "                x = layer(x, indices_list.pop(), pad_list.pop())\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return code, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae1314",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47f2e4c9",
   "metadata": {},
   "source": [
    "## TRAIN AUTOENCODER ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b4b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAE(model, train_loader, criterion, optimizer, n_epochs):\n",
    "    \"\"\"\n",
    "    Method used to train an AutoEncoder\n",
    "\n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        train_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "        optimizer: (torch.optim) an optimization algorithm\n",
    "        n_epochs: (int) number of epochs performed during training\n",
    "\n",
    "    Returns:\n",
    "        best_model: (nn.Module) the trained neural network.\n",
    "    \"\"\"\n",
    "    best_model = deepcopy(model)\n",
    "    train_best_loss = np.inf\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loader.dataset.train()\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            # ToDo\n",
    "            # Complete the training function in a similar way\n",
    "            # than for the CNN classification training.\n",
    "            # Retrieve mini-batch\n",
    "            images, labels = data['image'], data['label'] #.cuda(), data['label'].cuda()\n",
    "            # Forward pass + loss computation\n",
    "            _, outputs = model((images))\n",
    "            loss = criterion(outputs, images)\n",
    "            # Back-propagation\n",
    "            loss.backward()\n",
    "            # Parameters update\n",
    "            optimizer.step()\n",
    "            # Erase previous gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        mean_loss = testAE(model, train_loader, criterion)\n",
    "\n",
    "        print(f'Epoch {epoch}: loss = {mean_loss:.6f}')\n",
    "\n",
    "        if mean_loss < train_best_loss:\n",
    "            best_model = deepcopy(model)\n",
    "            train_best_loss = mean_loss\n",
    "\n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e56112",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def testAE(model, data_loader, criterion):\n",
    "    \"\"\"\n",
    "    Method used to test an AutoEncoder\n",
    "\n",
    "    Args:\n",
    "        model: (nn.Module) the neural network\n",
    "        data_loader: (DataLoader) a DataLoader wrapping a MRIDataset\n",
    "        criterion: (nn.Module) a method to compute the loss of a mini-batch of images\n",
    "\n",
    "    Returns:\n",
    "        results_df: (DataFrame) the label predicted for every subject\n",
    "        results_metrics: (dict) a set of metrics\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    data_loader.dataset.eval()\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(data_loader, 0):\n",
    "            images, labels = data['image'], data['label'] #.cuda(), data['label'].cuda()\n",
    "            _, outputs = model((images))\n",
    "            loss = criterion(outputs, images)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader.dataset) / np.product(data_loader.dataset.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee05844",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 10**-3\n",
    "n_epochs = 30\n",
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a510e082",
   "metadata": {},
   "outputs": [],
   "source": [
    "AELeftHC = AutoEncoder()#.cuda()\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(AELeftHC.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4ff11",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_AELeftHC = trainAE(AELeftHC, train_loaderLeftHC, criterion, optimizer, n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfbc001",
   "metadata": {},
   "source": [
    "## VISUALIZATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0ec8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nibabel as nib\n",
    "from scipy.ndimage import rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de70e3",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "subject = 'sub-OASIS10003'\n",
    "preprocessed_pt = torch.load(f'/Users/camille.brianceau/Downloads/OASIS-1_dataset/CAPS/subjects/{subject}/ses-M00/' +\n",
    "                    'deeplearning_prepare_data/image_based/custom/' + subject +\n",
    "                    '_ses-M00_'+\n",
    "                    'T1w_segm-graymatter_space-Ixi549Space_modulated-off_' +\n",
    "                    'probability.pt')\n",
    "input_pt = CropLeftHC()(preprocessed_pt).unsqueeze(0)#.cuda()\n",
    "_, output_pt = best_AELeftHC(input_pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e456d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = input_pt[0, 0, 15, :, :].cpu()\n",
    "slice_1 = input_pt[0, 0, :, 20, :].cpu()\n",
    "slice_2 = input_pt[0, 0, :, :, 15].cpu()\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(f'Center slices of the input image of subject {subject}')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/1.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65dcd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_0 = output_pt[0, 0, 15, :, :].cpu().detach()\n",
    "slice_1 = output_pt[0, 0, :, 20, :].cpu().detach()\n",
    "slice_2 = output_pt[0, 0, :, :, 15].cpu().detach()\n",
    "show_slices([slice_0, slice_1, slice_2])\n",
    "plt.suptitle(f'Center slices of the output image of subject {subject}')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/2.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b19b20",
   "metadata": {},
   "source": [
    "## CLUSTERING ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c66c809",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compute_dataset_features(data_loader, model):\n",
    "\n",
    "    concat_codes = torch.Tensor() #.cuda()\n",
    "    concat_labels = torch.LongTensor()\n",
    "    concat_names = []\n",
    "\n",
    "    for data in data_loader:\n",
    "      image = data['image'] #.cuda()\n",
    "      labels = data['label']\n",
    "      names = data['participant_id']\n",
    "\n",
    "      code, _ = model(image)\n",
    "      concat_codes = torch.cat([concat_codes, code.squeeze(1)], 0)\n",
    "      concat_labels = torch.cat([concat_labels, labels])\n",
    "      concat_names = concat_names + names\n",
    "\n",
    "    concat_codes_np = concat_codes.cpu().detach().numpy()\n",
    "    concat_labels_np = concat_labels.numpy()\n",
    "    concat_names = np.array(concat_names)[:, np.newaxis]\n",
    "\n",
    "    return concat_codes_np, concat_labels_np, concat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3afd46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_codes, train_labels, names = compute_dataset_features(train_loaderBothHC, best_AEBothHC)\n",
    "train_codes, train_labels, names = compute_dataset_features(train_loaderLeftHC, best_AELeftHC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07648042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import mixture\n",
    "from sklearn.metrics import adjusted_rand_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629526ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "model = mixture.GaussianMixture(n_components)\n",
    "model.fit(train_codes)\n",
    "train_predict = model.predict(train_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46a353e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = compute_metrics(train_labels, train_predict)\n",
    "ari = adjusted_rand_score(train_labels, train_predict)\n",
    "print(f\"Adjusted random index: {ari}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_np = np.concatenate([names, train_codes,\n",
    "                          train_labels[:, np.newaxis],\n",
    "                          train_predict[:, np.newaxis]], axis=1)\n",
    "columns = ['feature %i' % i for i in range(train_codes.shape[1])]\n",
    "columns = ['participant_id'] + columns + ['true_label', 'predicted_label']\n",
    "data_df = pd.DataFrame(data_np, columns=columns).set_index('participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d89a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = data_df.merge(OASIS_df.set_index('participant_id'), how='inner', on='participant_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13968ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Clustering values according to age and MMS score')\n",
    "for component in range(n_components):\n",
    "    predict_df = merged_df[merged_df.predicted_label == str(component)]\n",
    "    plt.plot(predict_df['age_bl'], predict_df['MMS'], 'o', label=f\"cluster {component}\")\n",
    "plt.legend()\n",
    "plt.xlabel('age')\n",
    "plt.ylabel('MMS')\n",
    "plt.savefig(\"/Users/camille.brianceau/aramis/NOW-2023/figures/6.png\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
